{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbd31b7-523a-4b48-a658-8792774810ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Configuration\n",
    "SCALES: List[float] = [0.3 + 0.1 * i for i in range(13)]  # 0.3 to 1.5\n",
    "ANGLES: List[int] = [0]\n",
    "CACHE_DIR = \"./data/scaled_mnist_10000\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Device (e.g., 'cuda' or 'cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_basis_vector(phi: torch.Tensor) -> torch.Tensor:\n",
    "    width, height = phi.shape\n",
    "    k = torch.arange(width, device=phi.device) - width // 2\n",
    "    j = torch.arange(height, device=phi.device) - height // 2\n",
    "    X, Y = torch.meshgrid(k, j, indexing=\"ij\")\n",
    "    B = [\n",
    "        torch.ones_like(phi),\n",
    "        X,\n",
    "        Y,\n",
    "        0.5 * X**2,\n",
    "        0.5 * Y**2,\n",
    "        0.5 * X * Y,\n",
    "        (X**2 * Y) / 6.0,\n",
    "        (X * Y**2) / 6.0, \n",
    "        (X * Y**3) / 24.0,\n",
    "        (X**2 * Y**2) / 24.0,\n",
    "        (Y * X**3) / 24.0,\n",
    "    ]\n",
    "    return torch.stack([(b * phi).sum() for b in B])\n",
    "\n",
    "# Precompute and cache dataset\n",
    "def get_or_create_augmented_tensor_dataset(train: bool = True, limit: int | None = None, overwrite_cache: bool = False) -> TensorDataset:\n",
    "    split_name = \"train\" if train else \"test\"\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"augmented_mnist_{split_name}.pt\")\n",
    "\n",
    "    if os.path.exists(cache_path) and not overwrite_cache:\n",
    "        print(f\"Loading cached dataset from: {cache_path}\")\n",
    "        dataset = torch.load(cache_path)\n",
    "        return TensorDataset(dataset.tensors[0].to(device), dataset.tensors[1].to(device))\n",
    "\n",
    "    print(f\"Creating augmented dataset for MNIST ({split_name})...\")\n",
    "    mnist_ds = MNIST(root=\"./data\", train=train, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for idx, (tensor_img, label) in enumerate(mnist_ds):\n",
    "        if limit is not None and idx >= limit:\n",
    "            break\n",
    "\n",
    "        for scale in SCALES:\n",
    "            for angle in ANGLES:\n",
    "                transformed = F.affine(\n",
    "                    tensor_img,\n",
    "                    angle=angle,\n",
    "                    translate=(0, 0),\n",
    "                    scale=scale,\n",
    "                    shear=(0.0, 0.0),\n",
    "                    interpolation=InterpolationMode.NEAREST,\n",
    "                    fill=0.0,\n",
    "                    center=None,\n",
    "                )\n",
    "                phi = transformed.squeeze(0).to(device)  # (28, 28)\n",
    "                vec = compute_basis_vector(phi)\n",
    "                X_list.append(vec)\n",
    "                y_list.append(label)\n",
    "\n",
    "    X_tensor = torch.stack(X_list)\n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.long, device=device)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    torch.save(dataset, cache_path)\n",
    "    print(f\"Saved dataset to: {cache_path}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create data loaders\n",
    "def get_data_loaders(batch_size: int = 512, limit: int | None = None, overwrite_cache: bool = False):\n",
    "    train_ds = get_or_create_augmented_tensor_dataset(train=True, limit=limit, overwrite_cache=overwrite_cache)\n",
    "    test_ds = get_or_create_augmented_tensor_dataset(train=False, limit=limit, overwrite_cache=overwrite_cache)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b6c502-615c-492d-8639-0215c681b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset for MNIST (train)...\n",
      "Saved dataset to: ./data/scaled_mnist_1000/augmented_mnist_train.pt\n",
      "Creating augmented dataset for MNIST (test)...\n",
      "Saved dataset to: ./data/scaled_mnist_1000/augmented_mnist_test.pt\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_data_loaders(batch_size=512, limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d410ea2-f914-48a9-ad45-c50d9792838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            outputs = model(x)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            correct += (pred_labels == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# --- Training loop\n",
    "def train_models(num_epochs=1000, early_stop_patience=200):\n",
    "    best_hnn_acc, best_mlp_acc = 0.0, 0.0\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_h, test_loss_h = [], []\n",
    "    train_loss_m, test_loss_m = [], []\n",
    "    train_acc_h, test_acc_h = [], []\n",
    "    train_acc_m, test_acc_m = [], []\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
    "        model_hnn.train()\n",
    "        model_mlp.train()\n",
    "        epoch_loss_h, epoch_loss_m = 0.0, 0.0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # HNN\n",
    "            opt_hnn.zero_grad()\n",
    "            out_hnn = model_hnn(x_batch)\n",
    "            loss_h = criterion(out_hnn, y_batch)\n",
    "            loss_h.backward()\n",
    "            opt_hnn.step()\n",
    "            epoch_loss_h += loss_h.item()\n",
    "\n",
    "            # MLP\n",
    "            opt_mlp.zero_grad()\n",
    "            out_mlp = model_mlp(x_batch)\n",
    "            loss_m = criterion(out_mlp, y_batch)\n",
    "            loss_m.backward()\n",
    "            opt_mlp.step()\n",
    "            epoch_loss_m += loss_m.item()\n",
    "\n",
    "        # Eval phase\n",
    "        def eval_model(model):\n",
    "            model.eval()\n",
    "            total_loss, correct, total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    preds = model(x)\n",
    "                    loss = criterion(preds, y)\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "            return total_loss / len(test_loader), correct / total\n",
    "\n",
    "        # Record losses/accuracies\n",
    "        train_loss_h.append(epoch_loss_h / len(train_loader))\n",
    "        train_loss_m.append(epoch_loss_m / len(train_loader))\n",
    "\n",
    "        val_loss_h, acc_h = eval_model(model_hnn)\n",
    "        val_loss_m, acc_m = eval_model(model_mlp)\n",
    "\n",
    "        test_loss_h.append(val_loss_h)\n",
    "        test_loss_m.append(val_loss_m)\n",
    "        test_acc_h.append(acc_h)\n",
    "        test_acc_m.append(acc_m)\n",
    "\n",
    "        train_acc_h.append(compute_accuracy(model_hnn, train_loader))\n",
    "        train_acc_m.append(compute_accuracy(model_mlp, train_loader))\n",
    "\n",
    "        # Logging\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"HNN Loss: {train_loss_h[-1]:.4f} / {val_loss_h:.4f}, \"\n",
    "                  f\"Acc: {train_acc_h[-1]*100:.2f}% / {acc_h*100:.2f}% || \"\n",
    "                  f\"MLP Loss: {train_loss_m[-1]:.4f} / {val_loss_m:.4f}, \"\n",
    "                  f\"Acc: {train_acc_m[-1]*100:.2f}% / {acc_m*100:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if acc_h > best_hnn_acc or acc_m > best_mlp_acc:\n",
    "            best_hnn_acc = max(best_hnn_acc, acc_h)\n",
    "            best_mlp_acc = max(best_mlp_acc, acc_m)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"train_loss_h\": train_loss_h, \"test_loss_h\": test_loss_h,\n",
    "        \"train_loss_m\": train_loss_m, \"test_loss_m\": test_loss_m,\n",
    "        \"train_acc_h\": train_acc_h, \"test_acc_h\": test_acc_h,\n",
    "        \"train_acc_m\": train_acc_m, \"test_acc_m\": test_acc_m\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1095eec2-724a-4fc6-bb55-618aade2c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hnn import HomogeneousNN\n",
    "from hnn_utils import initialize_weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11 = 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0, 4.0\n",
    "nu = 0.0\n",
    "Gd = torch.diag(torch.tensor([r1 + 2, r2 + 2, r3 + 2, r4 + 2, r5 + 2, r6 + 2, r7 + 2, r8 + 2, r9+2, r10+2, r11+2], device=device))\n",
    "P  = torch.eye(11, device=device)\n",
    "\n",
    "# --- Define models ---\n",
    "input_dim = 11\n",
    "hidden_layers = 1\n",
    "hidden_dim = 10\n",
    "output_dim = 10\n",
    "\n",
    "# 1) HomogeneousNN\n",
    "model_hnn = HomogeneousNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                           P=P, Gd=Gd, nu=nu, hidden_layers = hidden_layers).to(device)\n",
    "\n",
    "model_hnn.apply(initialize_weights)\n",
    "\n",
    "# MLP de référence pour la comparaison\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, hidden_layers = hidden_layers):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate MLP model\n",
    "model_mlp = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "model_mlp.apply(initialize_weights)\n",
    "\n",
    "# --- Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt_hnn = optim.Adam(model_hnn.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "opt_mlp = optim.Adam(model_mlp.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d49b4780-3a23-4211-8e97-370fb64d08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|                                  | 0/100 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Run training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- Plot results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_metrics\u001b[39m(results):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mtrain_models\u001b[39m\u001b[34m(num_epochs, early_stop_patience)\u001b[39m\n\u001b[32m     63\u001b[39m train_loss_h.append(epoch_loss_h / \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m     64\u001b[39m train_loss_m.append(epoch_loss_m / \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m val_loss_h, acc_h = \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_hnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m val_loss_m, acc_m = eval_model(model_mlp)\n\u001b[32m     69\u001b[39m test_loss_h.append(val_loss_h)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_models.<locals>.eval_model\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m         loss = criterion(preds, y)\n\u001b[32m     57\u001b[39m         total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/hnn.py:53\u001b[39m, in \u001b[36mHomogeneousNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.Gd_diag:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         S = \u001b[43mbatch_bisection_solve_diag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mV_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     x_sphere = dilation_batch_diag(\u001b[38;5;28mself\u001b[39m.lam, \u001b[38;5;28mself\u001b[39m.V, \u001b[38;5;28mself\u001b[39m.V_inv, -S, x)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/hnn_utils.py:285\u001b[39m, in \u001b[36mbatch_bisection_solve_diag\u001b[39m\u001b[34m(lam, V, V_inv, P, X, alpha, beta, tol, max_iter)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Compute norm of d(-s_mid)(X)\u001b[39;00m\n\u001b[32m    284\u001b[39m ds_mid = dilation_batch_diag(lam, V, V_inv, -s_mid, X)  \u001b[38;5;66;03m# (B, n)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m norm_ds_mid = \u001b[43mnorm_P_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# mask: True if norm_ds_mid < 1 -> we move s_max downward\u001b[39;00m\n\u001b[32m    288\u001b[39m mask = norm_ds_mid < \u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/hnn_utils.py:76\u001b[39m, in \u001b[36mnorm_P_batch\u001b[39m\u001b[34m(X, P)\u001b[39m\n\u001b[32m     74\u001b[39m PX = X @ P\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Elementwise product and sum across dim=1 -> (B,)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m quad_forms = \u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mPX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.sqrt(quad_forms)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Run training\n",
    "results = train_models(num_epochs=100)\n",
    "\n",
    "# --- Plot results\n",
    "def plot_metrics(results):\n",
    "    epochs = range(1, len(results[\"train_loss_h\"]) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_loss_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_loss_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_loss_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_loss_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CrossEntropy Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_acc_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_acc_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_acc_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_acc_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4639d0-a52e-47bf-aabb-7f716cefe9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
