{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbd31b7-523a-4b48-a658-8792774810ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Configuration\n",
    "SCALES: List[float] = [0.3 + 0.1 * i for i in range(13)]  # 0.3 to 1.5\n",
    "ANGLES: List[int] = [0]\n",
    "CACHE_DIR = \"./data/scaled_mnist_1000\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Device (e.g., 'cuda' or 'cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image -> 8D vector basis transformation (Pure Torch)\n",
    "def compute_basis_vector(phi: torch.Tensor) -> torch.Tensor:\n",
    "    width, height = phi.shape\n",
    "    k = torch.arange(width, device=phi.device) - width // 2\n",
    "    j = torch.arange(height, device=phi.device) - height // 2\n",
    "    X, Y = torch.meshgrid(k, j, indexing=\"ij\")\n",
    "    B = [\n",
    "        torch.ones_like(phi),\n",
    "        X,\n",
    "        Y,\n",
    "        0.5 * X**2,\n",
    "        0.5 * Y**2,\n",
    "        0.5 * X * Y,\n",
    "        (X**2 * Y) / 6.0,\n",
    "        (X * Y**2) / 6.0, \n",
    "        (X * Y**3) / 24.0,\n",
    "        (X**2 * Y**2) / 24.0,\n",
    "        (Y * X**3) / 24.0,\n",
    "    ]\n",
    "    return torch.stack([(b * phi).sum() for b in B])\n",
    "\n",
    "# Precompute and cache dataset\n",
    "def get_or_create_augmented_tensor_dataset(train: bool = True, limit: int | None = None, overwrite_cache: bool = False) -> TensorDataset:\n",
    "    split_name = \"train\" if train else \"test\"\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"augmented_mnist_{split_name}.pt\")\n",
    "\n",
    "    if os.path.exists(cache_path) and not overwrite_cache:\n",
    "        print(f\"Loading cached dataset from: {cache_path}\")\n",
    "        dataset = torch.load(cache_path)\n",
    "        return TensorDataset(dataset.tensors[0].to(device), dataset.tensors[1].to(device))\n",
    "\n",
    "    print(f\"Creating augmented dataset for MNIST ({split_name})...\")\n",
    "    mnist_ds = MNIST(root=\"./data\", train=train, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for idx, (tensor_img, label) in enumerate(mnist_ds):\n",
    "        if limit is not None and idx >= limit:\n",
    "            break\n",
    "\n",
    "        for scale in SCALES:\n",
    "            for angle in ANGLES:\n",
    "                transformed = F.affine(\n",
    "                    tensor_img,\n",
    "                    angle=angle,\n",
    "                    translate=(0, 0),\n",
    "                    scale=scale,\n",
    "                    shear=(0.0, 0.0),\n",
    "                    interpolation=InterpolationMode.NEAREST,\n",
    "                    fill=0.0,\n",
    "                    center=None,\n",
    "                )\n",
    "                phi = transformed.squeeze(0).to(device)  # (28, 28)\n",
    "                vec = compute_basis_vector(phi)\n",
    "                X_list.append(vec)\n",
    "                y_list.append(label)\n",
    "\n",
    "    X_tensor = torch.stack(X_list)\n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.long, device=device)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    torch.save(dataset, cache_path)\n",
    "    print(f\"Saved dataset to: {cache_path}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create data loaders\n",
    "def get_data_loaders(batch_size: int = 512, limit: int | None = None, overwrite_cache: bool = False):\n",
    "    train_ds = get_or_create_augmented_tensor_dataset(train=True, limit=limit, overwrite_cache=overwrite_cache)\n",
    "    test_ds = get_or_create_augmented_tensor_dataset(train=False, limit=limit, overwrite_cache=overwrite_cache)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b6c502-615c-492d-8639-0215c681b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset for MNIST (train)...\n",
      "Saved dataset to: ./data/scaled_mnist_1000/augmented_mnist_train.pt\n",
      "Creating augmented dataset for MNIST (test)...\n",
      "Saved dataset to: ./data/scaled_mnist_1000/augmented_mnist_test.pt\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_data_loaders(batch_size=512, limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1095eec2-724a-4fc6-bb55-618aade2c895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleMLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (9): Tanh()\n",
       "    (10): Linear(in_features=10, out_features=10, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hnn import HomogeneousNN\n",
    "from hnn_utils import initialize_weights\n",
    "import torch.nn as nn\n",
    "\n",
    "r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11 = 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0, 4.0\n",
    "nu = 0.0\n",
    "Gd = torch.diag(torch.tensor([r1 + 2, r2 + 2, r3 + 2, r4 + 2, r5 + 2, r6 + 2, r7 + 2, r8 + 2, r9+2, r10+2, r11+2], device=device))\n",
    "P  = torch.eye(11, device=device)\n",
    "\n",
    "# --- Define models ---\n",
    "input_dim = 11\n",
    "hidden_layers = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 10\n",
    "\n",
    "# 1) HomogeneousNN\n",
    "model_hnn = HomogeneousNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                           P=P, Gd=Gd, nu=nu, hidden_layers = hidden_layers).to(device)\n",
    "\n",
    "model_hnn.apply(initialize_weights)\n",
    "\n",
    "# MLP de référence pour la comparaison\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, hidden_layers = hidden_layers):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate MLP model\n",
    "model_mlp = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "model_mlp.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d410ea2-f914-48a9-ad45-c50d9792838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt_hnn = optim.Adam(model_hnn.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "opt_mlp = optim.Adam(model_mlp.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# --- Training loop\n",
    "def train_models(num_epochs=1000, early_stop_patience=200):\n",
    "    best_hnn_acc, best_mlp_acc = 0.0, 0.0\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_h, test_loss_h = [], []\n",
    "    train_loss_m, test_loss_m = [], []\n",
    "    train_acc_h, test_acc_h = [], []\n",
    "    train_acc_m, test_acc_m = [], []\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
    "        model_hnn.train()\n",
    "        model_mlp.train()\n",
    "        epoch_loss_h, epoch_loss_m = 0.0, 0.0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # HNN\n",
    "            opt_hnn.zero_grad()\n",
    "            out_hnn = model_hnn(x_batch)\n",
    "            loss_h = criterion(out_hnn, y_batch)\n",
    "            loss_h.backward()\n",
    "            opt_hnn.step()\n",
    "            epoch_loss_h += loss_h.item()\n",
    "\n",
    "            # MLP\n",
    "            opt_mlp.zero_grad()\n",
    "            out_mlp = model_mlp(x_batch)\n",
    "            loss_m = criterion(out_mlp, y_batch)\n",
    "            loss_m.backward()\n",
    "            opt_mlp.step()\n",
    "            epoch_loss_m += loss_m.item()\n",
    "\n",
    "        # Eval phase\n",
    "        def eval_model(model):\n",
    "            model.eval()\n",
    "            total_loss, correct, total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    preds = model(x)\n",
    "                    loss = criterion(preds, y)\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "            return total_loss / len(test_loader), correct / total\n",
    "\n",
    "        # Record losses/accuracies\n",
    "        train_loss_h.append(epoch_loss_h / len(train_loader))\n",
    "        train_loss_m.append(epoch_loss_m / len(train_loader))\n",
    "\n",
    "        val_loss_h, acc_h = eval_model(model_hnn)\n",
    "        val_loss_m, acc_m = eval_model(model_mlp)\n",
    "\n",
    "        test_loss_h.append(val_loss_h)\n",
    "        test_loss_m.append(val_loss_m)\n",
    "        test_acc_h.append(acc_h)\n",
    "        test_acc_m.append(acc_m)\n",
    "\n",
    "        train_acc_h.append(compute_accuracy(model_hnn, train_loader))\n",
    "        train_acc_m.append(compute_accuracy(model_mlp, train_loader))\n",
    "\n",
    "        # Logging\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"HNN Loss: {train_loss_h[-1]:.4f} / {val_loss_h:.4f}, \"\n",
    "                  f\"Acc: {train_acc_h[-1]*100:.2f}% / {acc_h*100:.2f}% || \"\n",
    "                  f\"MLP Loss: {train_loss_m[-1]:.4f} / {val_loss_m:.4f}, \"\n",
    "                  f\"Acc: {train_acc_m[-1]*100:.2f}% / {acc_m*100:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if acc_h > best_hnn_acc or acc_m > best_mlp_acc:\n",
    "            best_hnn_acc = max(best_hnn_acc, acc_h)\n",
    "            best_mlp_acc = max(best_mlp_acc, acc_m)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"train_loss_h\": train_loss_h, \"test_loss_h\": test_loss_h,\n",
    "        \"train_loss_m\": train_loss_m, \"test_loss_m\": test_loss_m,\n",
    "        \"train_acc_h\": train_acc_h, \"test_acc_h\": test_acc_h,\n",
    "        \"train_acc_m\": train_acc_m, \"test_acc_m\": test_acc_m\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49b4780-3a23-4211-8e97-370fb64d08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|                                   | 0/10 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Run training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- Plot results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_metrics\u001b[39m(results):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_models\u001b[39m\u001b[34m(num_epochs, early_stop_patience)\u001b[39m\n\u001b[32m     66\u001b[39m test_acc_h.append(acc_h)\n\u001b[32m     67\u001b[39m test_acc_m.append(acc_m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m train_acc_h.append(\u001b[43mcompute_accuracy\u001b[49m(model_hnn, train_loader))\n\u001b[32m     70\u001b[39m train_acc_m.append(compute_accuracy(model_mlp, train_loader))\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Logging\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'compute_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Run training\n",
    "results = train_models(num_epochs=10)\n",
    "\n",
    "# --- Plot results\n",
    "def plot_metrics(results):\n",
    "    epochs = range(1, len(results[\"train_loss_h\"]) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_loss_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_loss_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_loss_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_loss_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CrossEntropy Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_acc_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_acc_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_acc_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_acc_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19839a47-ce7c-4247-a641-0ffe5a29d28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
