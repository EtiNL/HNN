{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2de5ef31-3c3d-4e2b-9a0b-d5f4f06037c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List, Callable\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from math import factorial\n",
    "from itertools import product\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate basis functions b_i(x, y) = x^p y^q / r! with p+q=r <= MAX_DEGREE\n",
    "def generate_homogeneous_basis(max_degree: int):\n",
    "    basis = []\n",
    "    degrees = []  # (r = p + q)\n",
    "    for r in range(max_degree + 1):\n",
    "        for p in range(r + 1):\n",
    "            q = r - p\n",
    "            def monomial(p=p, q=q, r=r):\n",
    "                return lambda X, Y: (X**p * Y**q) / factorial(r)\n",
    "            basis.append(monomial())\n",
    "            degrees.append(r)\n",
    "    return basis, degrees\n",
    "\n",
    "# Compute basis vector from image phi\n",
    "def compute_basis_vector(phi: torch.Tensor) -> torch.Tensor:\n",
    "    width, height = phi.shape\n",
    "    k = torch.arange(width, device=phi.device) - width // 2\n",
    "    j = torch.arange(height, device=phi.device) - height // 2\n",
    "    X, Y = torch.meshgrid(k, j, indexing=\"ij\")\n",
    "    return torch.stack([(b(X, Y) * phi).sum() for b in BASIS])\n",
    "\n",
    "# Precompute and cache dataset\n",
    "def get_or_create_augmented_tensor_dataset(train: bool = True, limit: int | None = None, overwrite_cache: bool = False) -> TensorDataset:\n",
    "    split_name = \"train\" if train else \"test\"\n",
    "    limit_str = f\"{limit}\" if limit is not None else \"all\"\n",
    "    CACHE_DIR = f\"./data/scaled_mnist_limit{limit_str}_{split_name}_BaseDegree{MAX_DEGREE}\"\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"augmented_mnist_{split_name}.pt\")\n",
    "\n",
    "    if os.path.exists(cache_path) and not overwrite_cache:\n",
    "        print(f\"Loading cached dataset from: {cache_path}\")\n",
    "        dataset = torch.load(cache_path)\n",
    "        return TensorDataset(dataset.tensors[0].to(device), dataset.tensors[1].to(device))\n",
    "\n",
    "    print(f\"Creating augmented dataset for MNIST ({split_name})...\")\n",
    "    mnist_ds = MNIST(root=\"./data\", train=train, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for idx, (tensor_img, label) in enumerate(mnist_ds):\n",
    "        if limit is not None and idx >= limit:\n",
    "            break\n",
    "\n",
    "        for scale in SCALES:\n",
    "            for angle in ANGLES:\n",
    "                transformed = F.affine(\n",
    "                    tensor_img,\n",
    "                    angle=angle,\n",
    "                    translate=(0, 0),\n",
    "                    scale=scale,\n",
    "                    shear=(0.0, 0.0),\n",
    "                    interpolation=InterpolationMode.NEAREST,\n",
    "                    fill=0.0,\n",
    "                    center=None,\n",
    "                )\n",
    "                phi = transformed.squeeze(0).to(device)  # (28, 28)\n",
    "                vec = compute_basis_vector(phi)\n",
    "                X_list.append(vec)\n",
    "                y_list.append(label)\n",
    "\n",
    "    X_tensor = torch.stack(X_list)\n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.long, device=device)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    torch.save(dataset, cache_path)\n",
    "    print(f\"Saved dataset to: {cache_path}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create data loaders\n",
    "def get_data_loaders(batch_size: int = 512, limit: int | None = None, overwrite_cache: bool = False):\n",
    "    train_ds = get_or_create_augmented_tensor_dataset(train=True, limit=limit, overwrite_cache=overwrite_cache)\n",
    "    test_ds = get_or_create_augmented_tensor_dataset(train=False, limit=limit, overwrite_cache=overwrite_cache)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9506b1c0-fff4-435b-b5c8-6c6073959438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SCALES: List[float] = [0.3 + 0.1 * i for i in range(13)]  # 0.3 to 1.5\n",
    "ANGLES: List[int] = [0]\n",
    "MAX_DEGREE = 6 # Degree of the basis functions\n",
    "\n",
    "BASIS, DEGREE_INFO = generate_homogeneous_basis(MAX_DEGREE)\n",
    "\n",
    "print(len(BASIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1222b935-695b-4e24-9831-6b3c538755d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset for MNIST (train)...\n",
      "Saved dataset to: ./data/scaled_mnist_limit10000_train_BaseDegree6/augmented_mnist_train.pt\n",
      "Creating augmented dataset for MNIST (test)...\n",
      "Saved dataset to: ./data/scaled_mnist_limit10000_test_BaseDegree6/augmented_mnist_test.pt\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_data_loaders(batch_size=512, limit=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be87ae-2d40-4684-84d7-c1162525aa86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9720202a-7c79-4f2a-b6ed-0561cead1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            outputs = model(x)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            correct += (pred_labels == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# --- Training loop\n",
    "def train_models(num_epochs=1000, early_stop_patience=200):\n",
    "    best_hnn_acc, best_mlp_acc = 0.0, 0.0\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_h, test_loss_h = [], []\n",
    "    train_loss_m, test_loss_m = [], []\n",
    "    train_acc_h, test_acc_h = [], []\n",
    "    train_acc_m, test_acc_m = [], []\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
    "        model_hnn.train()\n",
    "        model_mlp.train()\n",
    "        epoch_loss_h, epoch_loss_m = 0.0, 0.0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # HNN\n",
    "            opt_hnn.zero_grad()\n",
    "            out_hnn = model_hnn(x_batch)\n",
    "            loss_h = criterion(out_hnn, y_batch)\n",
    "            loss_h.backward()\n",
    "            opt_hnn.step()\n",
    "            epoch_loss_h += loss_h.item()\n",
    "\n",
    "            # MLP\n",
    "            opt_mlp.zero_grad()\n",
    "            out_mlp = model_mlp(x_batch)\n",
    "            loss_m = criterion(out_mlp, y_batch)\n",
    "            loss_m.backward()\n",
    "            opt_mlp.step()\n",
    "            epoch_loss_m += loss_m.item()\n",
    "\n",
    "        # Eval phase\n",
    "        def eval_model(model):\n",
    "            model.eval()\n",
    "            total_loss, correct, total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    preds = model(x)\n",
    "                    loss = criterion(preds, y)\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "            return total_loss / len(test_loader), correct / total\n",
    "\n",
    "        # Record losses/accuracies\n",
    "        train_loss_h.append(epoch_loss_h / len(train_loader))\n",
    "        train_loss_m.append(epoch_loss_m / len(train_loader))\n",
    "\n",
    "        val_loss_h, acc_h = eval_model(model_hnn)\n",
    "        val_loss_m, acc_m = eval_model(model_mlp)\n",
    "\n",
    "        test_loss_h.append(val_loss_h)\n",
    "        test_loss_m.append(val_loss_m)\n",
    "        test_acc_h.append(acc_h)\n",
    "        test_acc_m.append(acc_m)\n",
    "\n",
    "        train_acc_h.append(compute_accuracy(model_hnn, train_loader))\n",
    "        train_acc_m.append(compute_accuracy(model_mlp, train_loader))\n",
    "\n",
    "        # Logging\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"HNN Loss: {train_loss_h[-1]:.4f} / {val_loss_h:.4f}, \"\n",
    "                  f\"Acc: {train_acc_h[-1]*100:.2f}% / {acc_h*100:.2f}% || \"\n",
    "                  f\"MLP Loss: {train_loss_m[-1]:.4f} / {val_loss_m:.4f}, \"\n",
    "                  f\"Acc: {train_acc_m[-1]*100:.2f}% / {acc_m*100:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if acc_h > best_hnn_acc or acc_m > best_mlp_acc:\n",
    "            best_hnn_acc = max(best_hnn_acc, acc_h)\n",
    "            best_mlp_acc = max(best_mlp_acc, acc_m)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"train_loss_h\": train_loss_h, \"test_loss_h\": test_loss_h,\n",
    "        \"train_loss_m\": train_loss_m, \"test_loss_m\": test_loss_m,\n",
    "        \"train_acc_h\": train_acc_h, \"test_acc_h\": test_acc_h,\n",
    "        \"train_acc_m\": train_acc_m, \"test_acc_m\": test_acc_m\n",
    "    }\n",
    "\n",
    "# --- Plot results\n",
    "def plot_metrics(results):\n",
    "    epochs = range(1, len(results[\"train_loss_h\"]) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_loss_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_loss_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_loss_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_loss_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CrossEntropy Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_acc_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_acc_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_acc_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_acc_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ca668-1b26-4416-bc41-2d0dcd2ebd0d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99872323-7e12-4318-a6e5-b2871fd3388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hnn import HomogeneousNN\n",
    "from hnn_utils import initialize_weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "r_list = torch.tensor(DEGREE_INFO, dtype=torch.float32, device=device)\n",
    "nu = 0.0\n",
    "Gd = torch.diag(r_list + 2)\n",
    "input_dim = len(BASIS)\n",
    "P = torch.eye(input_dim, device=device)\n",
    "\n",
    "# --- Define models ---\n",
    "hidden_layers = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 10\n",
    "\n",
    "# 1) HomogeneousNN\n",
    "model_hnn = HomogeneousNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                           P=P, Gd=Gd, nu=nu, hidden_layers = hidden_layers).to(device)\n",
    "\n",
    "model_hnn.apply(initialize_weights)\n",
    "\n",
    "# MLP de référence pour la comparaison\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, hidden_layers = hidden_layers):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate MLP model\n",
    "model_mlp = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "model_mlp.apply(initialize_weights)\n",
    "\n",
    "# --- Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt_hnn = optim.Adam(model_hnn.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "opt_mlp = optim.Adam(model_mlp.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7029dc7-b7c7-4824-bc27-f97665e44e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|                          | 1/300 [00:08<42:14,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: HNN Loss: 2.3063 / 2.3020, Acc: 9.38% / 8.96% || MLP Loss: 2.3804 / 2.3200, Acc: 14.32% / 14.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|▊                        | 10/300 [01:23<40:13,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: HNN Loss: 2.0319 / 2.0174, Acc: 24.96% / 23.48% || MLP Loss: 2.0504 / 2.0610, Acc: 23.09% / 22.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|█▋                       | 20/300 [02:46<38:37,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: HNN Loss: 1.8367 / 1.8362, Acc: 34.57% / 33.25% || MLP Loss: 1.8829 / 1.9001, Acc: 33.34% / 32.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|██▌                      | 30/300 [04:08<36:52,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: HNN Loss: 1.5495 / 1.5589, Acc: 45.26% / 44.48% || MLP Loss: 1.7408 / 1.7686, Acc: 38.26% / 36.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|███▎                     | 40/300 [05:32<36:03,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: HNN Loss: 1.4270 / 1.4497, Acc: 49.49% / 48.74% || MLP Loss: 1.6739 / 1.7061, Acc: 40.16% / 38.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  17%|████▏                    | 50/300 [06:56<35:11,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: HNN Loss: 1.3598 / 1.3892, Acc: 52.33% / 51.43% || MLP Loss: 1.6477 / 1.6821, Acc: 41.85% / 40.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|█████                    | 60/300 [08:18<32:57,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: HNN Loss: 1.3064 / 1.3356, Acc: 54.79% / 53.86% || MLP Loss: 1.6219 / 1.6593, Acc: 43.01% / 41.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  21%|█████▎                   | 64/300 [08:54<32:51,  8.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Run training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m plot_metrics(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mtrain_models\u001b[39m\u001b[34m(num_epochs, early_stop_patience)\u001b[39m\n\u001b[32m     63\u001b[39m train_loss_h.append(epoch_loss_h / \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m     64\u001b[39m train_loss_m.append(epoch_loss_m / \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m val_loss_h, acc_h = \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_hnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m val_loss_m, acc_m = eval_model(model_mlp)\n\u001b[32m     69\u001b[39m test_loss_h.append(val_loss_h)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_models.<locals>.eval_model\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m         loss = criterion(preds, y)\n\u001b[32m     57\u001b[39m         total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/hnn.py:53\u001b[39m, in \u001b[36mHomogeneousNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.Gd_diag:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         S = \u001b[43mbatch_bisection_solve_diag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mV_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     x_sphere = dilation_batch_diag(\u001b[38;5;28mself\u001b[39m.lam, \u001b[38;5;28mself\u001b[39m.V, \u001b[38;5;28mself\u001b[39m.V_inv, -S, x)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/hnn_utils.py:284\u001b[39m, in \u001b[36mbatch_bisection_solve_diag\u001b[39m\u001b[34m(lam, V, V_inv, P, X, alpha, beta, tol, max_iter)\u001b[39m\n\u001b[32m    281\u001b[39m s_mid = \u001b[32m0.5\u001b[39m * (s_min + s_max)\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Compute norm of d(-s_mid)(X)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m ds_mid = \u001b[43mdilation_batch_diag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_inv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43ms_mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, n)\u001b[39;00m\n\u001b[32m    285\u001b[39m norm_ds_mid = norm_P_batch(ds_mid, P)   \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# mask: True if norm_ds_mid < 1 -> we move s_max downward\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Univ_Lille/projet recherche/code/hnn_utils.py:130\u001b[39m, in \u001b[36mdilation_batch_diag\u001b[39m\u001b[34m(lam, V, V_inv, S, X)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdilation_batch_diag\u001b[39m(lam, V, V_inv, S, X):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     exp_lam = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, n)\u001b[39;00m\n\u001b[32m    131\u001b[39m     exp_matrices = (V[\u001b[38;5;28;01mNone\u001b[39;00m, ...] @ torch.diag_embed(exp_lam) @ V_inv[\u001b[38;5;28;01mNone\u001b[39;00m, ...]).real  \u001b[38;5;66;03m# (B,n,n)\u001b[39;00m\n\u001b[32m    132\u001b[39m     X_out = (torch.bmm(exp_matrices, X.unsqueeze(-\u001b[32m1\u001b[39m)).squeeze(-\u001b[32m1\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Run training\n",
    "results = train_models(num_epochs=300)\n",
    "plot_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c58e4-b91c-4042-a645-e60e01b4c7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
