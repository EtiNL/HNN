{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de5ef31-3c3d-4e2b-9a0b-d5f4f06037c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import List, Callable\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from math import factorial\n",
    "from itertools import product\n",
    "from tqdm import trange\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Generate basis functions b_i(x, y) = x^p y^q / r! with p+q=r <= MAX_DEGREE\n",
    "def generate_homogeneous_basis(max_degree: int):\n",
    "    basis = []\n",
    "    degrees = []  # (r = p + q)\n",
    "    for r in range(max_degree + 1):\n",
    "        for p in range(r + 1):\n",
    "            q = r - p\n",
    "            def monomial(p=p, q=q, r=r):\n",
    "                return lambda X, Y: (X**p * Y**q) / factorial(r)\n",
    "            basis.append(monomial())\n",
    "            degrees.append(r)\n",
    "    return basis, degrees\n",
    "\n",
    "# Compute basis vector from image phi\n",
    "def compute_basis_vector(phi: torch.Tensor) -> torch.Tensor:\n",
    "    width, height = phi.shape\n",
    "    k = torch.arange(width, device=phi.device) - width // 2\n",
    "    j = torch.arange(height, device=phi.device) - height // 2\n",
    "    X, Y = torch.meshgrid(k, j, indexing=\"ij\")\n",
    "    return torch.stack([(b(X, Y) * phi).sum() for b in BASIS])\n",
    "\n",
    "# Precompute and cache dataset\n",
    "def get_or_create_augmented_tensor_dataset(train: bool = True, limit = None, overwrite_cache: bool = False) -> TensorDataset:\n",
    "    split_name = \"train\" if train else \"test\"\n",
    "    limit_str = f\"{limit}\" if limit is not None else \"all\"\n",
    "    CACHE_DIR = f\"./data\"\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"scaled_mnist_limit{limit_str}_{split_name}_BaseDegree{MAX_DEGREE}.pt\")\n",
    "\n",
    "    if os.path.exists(cache_path) and not overwrite_cache:\n",
    "        print(f\"Loading cached dataset from: {cache_path}\")\n",
    "        dataset = torch.load(cache_path, weights_only=False)\n",
    "        return TensorDataset(dataset.tensors[0].to(device), dataset.tensors[1].to(device))\n",
    "\n",
    "    print(f\"Creating augmented dataset for MNIST ({split_name})...\")\n",
    "    mnist_ds = MNIST(root=\"./data\", train=train, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    num_samples = len(mnist_ds) if limit is None else min(limit, len(mnist_ds))\n",
    "    total_iterations = num_samples * len(SCALES) * len(ANGLES)\n",
    "\n",
    "    with trange(num_samples, desc=f\"Augmenting {split_name}\") as t:\n",
    "        for idx in t:\n",
    "\n",
    "            tensor_img, label = mnist_ds[idx]\n",
    "\n",
    "            for scale in SCALES:\n",
    "                for angle in ANGLES:\n",
    "                    transformed = F.affine(\n",
    "                        tensor_img,\n",
    "                        angle=angle,\n",
    "                        translate=(0, 0),\n",
    "                        scale=scale,\n",
    "                        shear=(0.0, 0.0),\n",
    "                        interpolation=InterpolationMode.NEAREST,\n",
    "                        fill=0.0,\n",
    "                        center=None,\n",
    "                    )\n",
    "                    phi = transformed.squeeze(0).to(device)  # (28, 28)\n",
    "                    vec = compute_basis_vector(phi)\n",
    "                    X_list.append(vec)\n",
    "                    y_list.append(label)\n",
    "\n",
    "    X_tensor = torch.stack(X_list)\n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.long, device=device)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    torch.save(dataset, cache_path)\n",
    "    print(f\"Saved dataset to: {cache_path}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "def get_data_loaders(batch_size: int = 512, limit = None, overwrite_cache: bool = False):\n",
    "    train_ds = get_or_create_augmented_tensor_dataset(train=True, limit=limit, overwrite_cache=overwrite_cache)\n",
    "    test_ds = get_or_create_augmented_tensor_dataset(train=False, limit=limit, overwrite_cache=overwrite_cache)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9506b1c0-fff4-435b-b5c8-6c6073959438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SCALES: List[float] = [0.3 + 0.1 * i for i in range(13)]  # 0.3 to 1.5\n",
    "ANGLES: List[int] = [0]\n",
    "MAX_DEGREE = 8 # Degree of the basis functions\n",
    "\n",
    "BASIS, DEGREE_INFO = generate_homogeneous_basis(MAX_DEGREE)\n",
    "\n",
    "print(len(BASIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222b935-695b-4e24-9831-6b3c538755d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating augmented dataset for MNIST (train)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train: 100%|██████████| 1000/1000 [00:35<00:00, 28.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to: ./data/scaled_mnist_limit1000_train_BaseDegree8.pt\n",
      "Creating augmented dataset for MNIST (test)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting test:  60%|██████    | 603/1000 [00:21<00:13, 28.49it/s]"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_data_loaders(batch_size=4096, limit=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be87ae-2d40-4684-84d7-c1162525aa86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9720202a-7c79-4f2a-b6ed-0561cead1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            outputs = model(x)\n",
    "            pred_labels = torch.argmax(outputs, dim=1)\n",
    "            correct += (pred_labels == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# --- Training loop\n",
    "def train_models(num_epochs=1000, early_stop_patience=200):\n",
    "    best_hnn_acc, best_mlp_acc = 0.0, 0.0\n",
    "    patience = 0\n",
    "\n",
    "    train_loss_h, test_loss_h = [], []\n",
    "    train_loss_m, test_loss_m = [], []\n",
    "    train_acc_h, test_acc_h = [], []\n",
    "    train_acc_m, test_acc_m = [], []\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
    "        model_hnn.train()\n",
    "        model_mlp.train()\n",
    "        epoch_loss_h, epoch_loss_m = 0.0, 0.0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # HNN\n",
    "            opt_hnn.zero_grad()\n",
    "            out_hnn = model_hnn(x_batch)\n",
    "            loss_h = criterion(out_hnn, y_batch)\n",
    "            loss_h.backward()\n",
    "            opt_hnn.step()\n",
    "            epoch_loss_h += loss_h.item()\n",
    "\n",
    "            # MLP\n",
    "            opt_mlp.zero_grad()\n",
    "            out_mlp = model_mlp(x_batch)\n",
    "            loss_m = criterion(out_mlp, y_batch)\n",
    "            loss_m.backward()\n",
    "            opt_mlp.step()\n",
    "            epoch_loss_m += loss_m.item()\n",
    "\n",
    "        # Eval phase\n",
    "        def eval_model(model):\n",
    "            model.eval()\n",
    "            total_loss, correct, total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    preds = model(x)\n",
    "                    loss = criterion(preds, y)\n",
    "                    total_loss += loss.item()\n",
    "                    correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "                    total += y.size(0)\n",
    "            return total_loss / len(test_loader), correct / total\n",
    "\n",
    "        # Record losses/accuracies\n",
    "        train_loss_h.append(epoch_loss_h / len(train_loader))\n",
    "        train_loss_m.append(epoch_loss_m / len(train_loader))\n",
    "\n",
    "        val_loss_h, acc_h = eval_model(model_hnn)\n",
    "        val_loss_m, acc_m = eval_model(model_mlp)\n",
    "\n",
    "        test_loss_h.append(val_loss_h)\n",
    "        test_loss_m.append(val_loss_m)\n",
    "        test_acc_h.append(acc_h)\n",
    "        test_acc_m.append(acc_m)\n",
    "\n",
    "        train_acc_h.append(compute_accuracy(model_hnn, train_loader))\n",
    "        train_acc_m.append(compute_accuracy(model_mlp, train_loader))\n",
    "\n",
    "        # Logging\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"HNN Loss: {train_loss_h[-1]:.4f} / {val_loss_h:.4f}, \"\n",
    "                  f\"Acc: {train_acc_h[-1]*100:.2f}% / {acc_h*100:.2f}% || \"\n",
    "                  f\"MLP Loss: {train_loss_m[-1]:.4f} / {val_loss_m:.4f}, \"\n",
    "                  f\"Acc: {train_acc_m[-1]*100:.2f}% / {acc_m*100:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if acc_h > best_hnn_acc or acc_m > best_mlp_acc:\n",
    "            best_hnn_acc = max(best_hnn_acc, acc_h)\n",
    "            best_mlp_acc = max(best_mlp_acc, acc_m)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"train_loss_h\": train_loss_h, \"test_loss_h\": test_loss_h,\n",
    "        \"train_loss_m\": train_loss_m, \"test_loss_m\": test_loss_m,\n",
    "        \"train_acc_h\": train_acc_h, \"test_acc_h\": test_acc_h,\n",
    "        \"train_acc_m\": train_acc_m, \"test_acc_m\": test_acc_m\n",
    "    }\n",
    "\n",
    "# --- Plot results\n",
    "def plot_metrics(results):\n",
    "    epochs = range(1, len(results[\"train_loss_h\"]) + 1)\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_loss_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_loss_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_loss_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_loss_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CrossEntropy Loss\")\n",
    "    plt.title(\"Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, results[\"train_acc_h\"], label=\"HNN Train\")\n",
    "    plt.plot(epochs, results[\"test_acc_h\"], label=\"HNN Test\")\n",
    "    plt.plot(epochs, results[\"train_acc_m\"], label=\"MLP Train\", linestyle=\"--\")\n",
    "    plt.plot(epochs, results[\"test_acc_m\"], label=\"MLP Test\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ca668-1b26-4416-bc41-2d0dcd2ebd0d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99872323-7e12-4318-a6e5-b2871fd3388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hnn import HomogeneousNN\n",
    "from hnn_utils import initialize_weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "r_list = torch.tensor(DEGREE_INFO, dtype=torch.float32, device=device)\n",
    "nu = 0.0\n",
    "Gd = torch.diag(r_list + 2)\n",
    "input_dim = len(BASIS)\n",
    "P = torch.eye(input_dim, device=device)\n",
    "\n",
    "# --- Define models ---\n",
    "hidden_layers = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 10\n",
    "\n",
    "# 1) HomogeneousNN\n",
    "model_hnn = HomogeneousNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                           P=P, Gd=Gd, nu=nu, hidden_layers = hidden_layers).to(device)\n",
    "\n",
    "model_hnn.apply(initialize_weights)\n",
    "\n",
    "# MLP de référence pour la comparaison\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, hidden_layers = hidden_layers):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate MLP model\n",
    "model_mlp = SimpleMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "model_mlp.apply(initialize_weights)\n",
    "\n",
    "# --- Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt_hnn = optim.Adam(model_hnn.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "opt_mlp = optim.Adam(model_mlp.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7029dc7-b7c7-4824-bc27-f97665e44e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 1/1000 [00:08<2:29:24,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: HNN Loss: 2.3165 / 2.3142, Acc: 9.82% / 9.99% || MLP Loss: 2.4151 / 2.4161, Acc: 10.85% / 10.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   1%|          | 10/1000 [01:29<2:28:03,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: HNN Loss: 2.3004 / 2.3020, Acc: 10.36% / 9.79% || MLP Loss: 2.2682 / 2.2684, Acc: 17.87% / 16.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   2%|▏         | 20/1000 [02:59<2:26:39,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: HNN Loss: 2.2896 / 2.2905, Acc: 14.07% / 13.81% || MLP Loss: 2.1626 / 2.1627, Acc: 20.55% / 19.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|▎         | 30/1000 [04:29<2:25:08,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: HNN Loss: 2.2383 / 2.2328, Acc: 22.43% / 21.76% || MLP Loss: 2.0845 / 2.0877, Acc: 24.42% / 23.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   4%|▍         | 40/1000 [05:59<2:24:05,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: HNN Loss: 2.1272 / 2.1153, Acc: 21.78% / 21.13% || MLP Loss: 2.0216 / 2.0271, Acc: 26.50% / 25.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   5%|▌         | 50/1000 [07:29<2:22:14,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: HNN Loss: 2.0792 / 2.0668, Acc: 22.84% / 22.01% || MLP Loss: 1.9683 / 1.9753, Acc: 27.73% / 27.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f0574b146d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elanzera/HNN/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Training Epochs:   5%|▌         | 53/1000 [08:00<2:23:07,  9.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Run training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plot_metrics(results)\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(num_epochs, early_stop_patience)\u001b[0m\n\u001b[1;32m     64\u001b[0m train_loss_m\u001b[38;5;241m.\u001b[39mappend(epoch_loss_m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m     66\u001b[0m val_loss_h, acc_h \u001b[38;5;241m=\u001b[39m eval_model(model_hnn)\n\u001b[0;32m---> 67\u001b[0m val_loss_m, acc_m \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_mlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m test_loss_h\u001b[38;5;241m.\u001b[39mappend(val_loss_h)\n\u001b[1;32m     70\u001b[0m test_loss_m\u001b[38;5;241m.\u001b[39mappend(val_loss_m)\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36mtrain_models.<locals>.eval_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     52\u001b[0m total_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     55\u001b[0m         preds \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     56\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(preds, y)\n",
      "File \u001b[0;32m~/HNN/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/HNN/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/HNN/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/HNN/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/HNN/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:207\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HNN/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:207\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Run training\n",
    "results = train_models(num_epochs=1000)\n",
    "plot_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879b613-c54d-457e-bab9-abea7ebbc032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hnn-venv)",
   "language": "python",
   "name": "hnn-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
